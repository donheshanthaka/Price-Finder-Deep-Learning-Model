{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99d0fa9",
   "metadata": {},
   "source": [
    "# Price Finder Feature Extraction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ac7ea",
   "metadata": {},
   "source": [
    "This notebook contains the development of the CNN image classification model used in the Price Finder API to identify vehicles through images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab0f38",
   "metadata": {},
   "source": [
    "## A quick run-through of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a534195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_script import walk_through_dir\n",
    "\n",
    "# The path to the images folder that contains the train/test/validate image sets\n",
    "PATH = \"images\"\n",
    "\n",
    "walk_through_dir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a96e2d",
   "metadata": {},
   "source": [
    "## Image preprocessing\n",
    "\n",
    "Image preprocessing is done through converting the downloaded images into rgb color format since there can be instances where images of different types such as png to be automatically saved as jpg through the web scraper used, but in reality they would still contain that extra alpha channel making those images 4-channels. For the model that is developed for the Price Finder API, all the images should be converted in 3-channels. Therefore RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225b495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A script from the helper_script is used to convert the images\n",
    "\n",
    "from helper_script import convert_images_rgb\n",
    "\n",
    "convert_images_rgb(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45bf16d",
   "metadata": {},
   "source": [
    "## Create the data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae61669",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"images/train\"\n",
    "test_dir = \"images/test\"\n",
    "validation_dir = \"images/validate\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE,\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                crop_to_aspect_ratio=False)\n",
    "\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE,\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                shuffle=False,\n",
    "                                                                crop_to_aspect_ratio=False)\n",
    "\n",
    "validation_data = tf.keras.preprocessing.image_dataset_from_directory(validation_dir,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE,\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                shuffle=False,\n",
    "                                                                crop_to_aspect_ratio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the class names\n",
    "train_data.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946817f3",
   "metadata": {},
   "source": [
    "## Building a transfer learning feature extraction model using the Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c302c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_script import create_tensorboard_callback\n",
    "\n",
    "# 1. Create the base model with tf.keras.applications\n",
    "base_model = tf.keras.applications.EfficientNetB1(include_top = False)\n",
    "\n",
    "# 2. Freeze the base model (the underlying pre-trained patterns aren't updated during training)\n",
    "base_model.trainable = False\n",
    "\n",
    "# 3. Create inputs into our model\n",
    "inputs = tf.keras.layers.Input(shape = (224, 224, 3), name = \"input_layer\")\n",
    "\n",
    "# 4. Pass the inputs to the base_model\n",
    "x = base_model(inputs)\n",
    "print(f\"Shape after passing inputs through base model: {x.shape}\")\n",
    "\n",
    "# 5. Avergae pool the outputs of the base model (aggregate all the most important information, reduce the number of computations)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name = \"global_average_pooling_layer\")(x)\n",
    "print(f\"Shape after GlobalAveragePooling2D: {x.shape}\")\n",
    "\n",
    "# 6. Create the output activation layer\n",
    "outputs = tf.keras.layers.Dense(4, activation = \"softmax\", name = \"output_layer\")(x)\n",
    "\n",
    "# 7. Combine the inputs with the outputs into a model\n",
    "model_1 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# 8. Compile the model\n",
    "model_1.compile(loss = \"categorical_crossentropy\",\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "# 9. Fit the model and save its history\n",
    "history_1 = model_1.fit(train_data,\n",
    "                        epochs = 5,\n",
    "                        steps_per_epoch = len(train_data),\n",
    "                        validation_data = validation_data,\n",
    "                        validation_steps = len(validation_data),\n",
    "                        callbacks = [create_tensorboard_callback(dir_name = \"price_finder_model\",\n",
    "                                                                experiment_name = \"model_1_EfficientNetB1\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207262b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the full test dataset\n",
    "model_1.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6332586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ccdd75",
   "metadata": {},
   "source": [
    "## Evaluting the model based on training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1756d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_script import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(history_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f93fd",
   "metadata": {},
   "source": [
    "## Making predictions using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3934e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = loaded_model_1.predict(test_data, verbose=1) # set verbosity to see how long is left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b371c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of prediction probabilities for sample 0: {len(pred_probs[0])}\")\n",
    "print(f\"What prediction probability sample 0 looks like:\\n {pred_probs[0]}\")\n",
    "print(f\"The class with the highest predicted probability by the model for sample 0: {pred_probs[0].argmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get our test labels we need to unravel our test_data BatchDataset\n",
    "y_labels = []\n",
    "for images, labels in test_data.unbatch():\n",
    "    y_labels.append(labels.numpy().argmax()) # currently test labels look like: [0, 0, 0, 1, .... 0, 0], we want the index value where the \"1\" occurs\n",
    "y_labels[:10] # look at the first 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3a012",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55da0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(y_true=y_labels,\n",
    "                      y_pred=pred_classes,\n",
    "                      classes=class_names,\n",
    "                      figsize=(10, 10),\n",
    "                      text_size=20,\n",
    "                      savefig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c99185",
   "metadata": {},
   "source": [
    "# Visualizing predictions on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68469a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make preds on a series of random images\n",
    "import os\n",
    "import random\n",
    "\n",
    "plt.figure(figsize=(17, 25))\n",
    "for i in range(3):\n",
    "    \n",
    "    # Choose random image(s) from random class(es)\n",
    "    class_name = random.choice(class_names)\n",
    "    filename = random.choice(os.listdir(test_dir + \"/\" + class_name))\n",
    "    filepath = test_dir + \"/\" + class_name + \"/\" + filename\n",
    "\n",
    "    # Load the image and make predictions\n",
    "    img = load_and_prep_image(filepath, scale=False)\n",
    "    img_expanded = tf.expand_dims(img, axis=0)\n",
    "    # print(img_expanded.shape)\n",
    "    pred_prob = loaded_model_1.predict(img_expanded) # get prediction probabilities array\n",
    "    pred_class = class_names[pred_prob.argmax()] # get highest prediction probability index and match it class_names list\n",
    "    # print(pred_prob)\n",
    "    # print(pred_class)\n",
    "\n",
    "    # Plot the image(s)\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    # print(img)\n",
    "    plt.imshow(img/225.)\n",
    "    if class_name == pred_class: # if predicted class matches truth class, make text green\n",
    "        title_color = \"g\"\n",
    "    else:\n",
    "        title_color = \"r\"\n",
    "    plt.title(f\"actual: {class_name}, pred: {pred_class}, prob: {pred_prob.max():.2f}\", c=title_color, fontsize=8)\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c41901",
   "metadata": {},
   "source": [
    "## Finding the most wrong predictions\n",
    "\n",
    "Steps:\n",
    "\n",
    "   * Get all of the image file paths in the test dataset using list_files() method.\n",
    "\n",
    "   * Create a pandas DataFrame of the image filepaths, ground truth labels, predicted classes (from our model), max prediction probabilities, prediction class names, ground truth class names.\n",
    "\n",
    "   * Use our DataFrame to find all the wrong predictions (where the ground truth label doesn't match the prediction).\n",
    "\n",
    "   * Sort the DataFrame based on wrong predictions (have the highest prediction probability predictions at the top).\n",
    "\n",
    "   * Visualize the images with the highest prediction probabilities but have the wrong prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3eab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get all of the image file paths in the test dataset\n",
    "filepaths = []\n",
    "for filepath in test_data.list_files(\"images/test/*/*.jpg\",\n",
    "                                     shuffle=False):\n",
    "    filepaths.append(filepath.numpy())\n",
    "filepaths[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a DataFrame of different parameters for each of our test images\n",
    "import pandas as pd\n",
    "pred_df = pd.DataFrame({\"img_path\": filepaths,\n",
    "                        \"y_true\": y_labels,\n",
    "                        \"y_pred\": pred_classes,\n",
    "                        \"pred_conf\": pred_probs.max(axis=1), # get the maximum prediction probability value\n",
    "                        \"y_true_classname\": [class_names[i] for i in y_labels],\n",
    "                        \"y_pred_classname\": [class_names[i] for i in pred_classes]})\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Find out in our DataFrame which predictions are wrong\n",
    "pred_df[\"pred_correct\"] = pred_df[\"y_true\"] == pred_df[\"y_pred\"]\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbf8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sort our DataFrame to have most wrong predictions at the top\n",
    "top_100_wrong = pred_df[pred_df[\"pred_correct\"] == False].sort_values(\"pred_conf\", ascending=False)[:100]\n",
    "top_100_wrong.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0573ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of incorrect predictions: {len(top_100_wrong)} out of {len(pred_classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualize the test data samples which have the wrong prediction but highest pred probability\n",
    "images_to_view = 9\n",
    "start_index = 0\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, row in enumerate(top_100_wrong[start_index:start_index+images_to_view].itertuples()):\n",
    "  plt.subplot(3, 3, i+1)\n",
    "  img = load_and_prep_image(row[1], scale=False)\n",
    "  _, _, _, _, pred_prob, y_true_classname, y_pred_classname, _ = row # only interested in a few parameters of each row \"_\" is used to skip the ones you don't need\n",
    "  plt.imshow(img/255.)\n",
    "  plt.title(f\"actual: {y_true_classname}, pred: {y_pred_classname} \\nprob: {pred_prob}\")\n",
    "  plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d0106d",
   "metadata": {},
   "source": [
    "## Testing on custom images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the custom food images filepaths\n",
    "import os\n",
    "custom_images = [\"custom_images_new/\" + img_path for img_path in os.listdir(\"custom_images_new\")]\n",
    "custom_images[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba3cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on and plot custom food images\n",
    "for img in custom_images:\n",
    "    img = load_and_prep_image(img, scale=False) # don't need to scale for our EfficientNetB0 model\n",
    "    pred_prob = loaded_model_1.predict(tf.expand_dims(img, axis=0), verbose=0) # make prediction on image with shape [1, 224, 224, 3] (same shape as model was trained on)\n",
    "    pred_class = class_names[pred_prob.argmax()] # get the index with the highet prediction probability\n",
    "    # Plot the appropriate information\n",
    "    plt.figure()\n",
    "    plt.imshow(img/255.)\n",
    "    plt.title(f\"pred: {pred_class}, prob: {pred_prob.max():.2f}\")\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc44216",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "The tensorflow model is saved to be used later on the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_script import save_model\n",
    "\n",
    "save_model(model_1, \"model_1_EfficientNetB1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
