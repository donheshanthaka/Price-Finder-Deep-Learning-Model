{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99d0fa9",
   "metadata": {},
   "source": [
    "# Price Finder Feature Extraction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ac7ea",
   "metadata": {},
   "source": [
    "This notebook contains the development of the CNN image classification model used in the Price Finder API to identify vehicles through images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d850e2",
   "metadata": {},
   "source": [
    "## A quick run-through of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5da7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_script import walk_through_dir\n",
    "\n",
    "# The path to the images folder that contains the train/test/validate image sets\n",
    "PATH = \"images\"\n",
    "\n",
    "walk_through_dir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a96e2d",
   "metadata": {},
   "source": [
    "## Image preprocessing\n",
    "\n",
    "Image preprocessing is done through converting the downloaded images into rgb color format since there can be instances where images of different types such as png to be automatically saved as jpg through the web scraper used, but in reality they would still contain that extra alpha channel making those images 4-channels. For the model that is developed for the Price Finder API, all the images should be converted in 3-channels. Therefore RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225b495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A script from the helper_script is used to convert the images\n",
    "\n",
    "from helper_script import convert_images_rgb\n",
    "\n",
    "convert_images_rgb(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4539b",
   "metadata": {},
   "source": [
    "## Create the data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e15ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"images/train\"\n",
    "test_dir = \"images/test\"\n",
    "validation_dir = \"images/validate\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE,\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                crop_to_aspect_ratio=False)\n",
    "\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE,\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                shuffle=False,\n",
    "                                                                crop_to_aspect_ratio=False)\n",
    "\n",
    "validation_data = tf.keras.preprocessing.image_dataset_from_directory(validation_dir,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE,\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                shuffle=False,\n",
    "                                                                crop_to_aspect_ratio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00341fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the class names\n",
    "train_data.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d30c681",
   "metadata": {},
   "source": [
    "## Building a transfer learning feature extraction model using the Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13625e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the base model with tf.keras.applications\n",
    "base_model = tf.keras.applications.EfficientNetB1(include_top = False)\n",
    "\n",
    "# 2. Freeze the base model (the underlying pre-trained patterns aren't updated during training)\n",
    "base_model.trainable = False\n",
    "\n",
    "# 3. Create inputs into our model\n",
    "inputs = tf.keras.layers.Input(shape = (224, 224, 3), name = \"input_layer\")\n",
    "\n",
    "# 4. Pass the inputs to the base_model\n",
    "x = base_model(inputs)\n",
    "print(f\"Shape after passing inputs through base model: {x.shape}\")\n",
    "\n",
    "# 5. Avergae pool the outputs of the base model (aggregate all the most important information, reduce the number of computations)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name = \"global_average_pooling_layer\")(x)\n",
    "print(f\"Shape after GlobalAveragePooling2D: {x.shape}\")\n",
    "\n",
    "# 6. Create the output activation layer\n",
    "outputs = tf.keras.layers.Dense(4, activation = \"softmax\", name = \"output_layer\")(x)\n",
    "\n",
    "# 7. Combine the inputs with the outputs into a model\n",
    "model_1 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# 8. Compile the model\n",
    "model_1.compile(loss = \"categorical_crossentropy\",\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "# 9. Fit the model and save its history\n",
    "history_1 = model_1.fit(train_data,\n",
    "                        epochs = 5,\n",
    "                        steps_per_epoch = len(train_data),\n",
    "                        validation_data = validation_data,\n",
    "                        validation_steps = len(validation_data),\n",
    "                        callbacks = [create_tensorboard_callback(dir_name = \"price_finder_model\",\n",
    "                                                                experiment_name = \"model_1_EfficientNetB1\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf31d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
